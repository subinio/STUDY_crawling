# 파이썬을 이용한 크롤링

<br>

## 1. BeautifulSoup (requests or urllib)
- HTML, XML 파일의 정보를 추출해내는 파이썬 라이브러리이다.

- python 내장 모듈인 requests나 urllib을 이용해 HTML을 다운 받고, beautifulsoup으로 테이터를 추출한다(파싱).

- 서버에서 HTML을 다운 받기 때문에 서버사이드 렌더링을 사용하지 않는 SPA 사이트나, javascipt 렌더링을 필요로 하는 사이트들은 크롤링하기가 까다롭다.

- 하지만 사용하기 쉽고,  멀티프로세스나 멀티스레드를 사용할 경우, 속도도 매우 빠르다고 할 수 있다.

<br>

## 2. Selenium
- Selenium은 웹 자동화 테스트 (버튼 클릭, 스크롤 조작 등등)가 가능한 **동적 웹페이지**에서 사용되는 프레임워크이다.

- 셀레니움을 이용한 크롤러는 웹 페이지에서 javascript 렌더링을 통해 생성되는 데이터들을 손쉽게 가져올 수 있다.

- 인터넷 브라우저를 통해 크롤링을 하는 개념이라, 실제 보여지는 웹페이지의 전부를 가져올 수 있고, 디버깅 방법또한 직관적이다.

- 하지만 웹 브라우저를 실제로 실행시키는 방법이기 때문에 속도도 많이 느리고, 메모리도 상대적으로 많이 차지한다.

- 멀티프로세스를 사용해서 여러 브라우저로 크롤링하도록 하면 속도를 일정 부분 개선이 가능하다.

- 셀레니움으로 할 수 있는 것
  - 자동 로그인, 버튼 누르기, 동적 웹 크롤링

<br>

## 3. Scrapy
- Scrapy는 크롤링을 위해 개발된 프레임워크이다.
- 미들웨어, 파이프라인, javascript renderer(splash), proxy, xpath, CLI 등 다양한 기능들과 플러그인들을 사용할 수 있다.
- 병렬처리, robots.txt 준수 여부, 다운로드 속도 제어 등도 설정할 수 있다.
- Django 와 같은 백엔드 서비스와 연동하기도 좋다.
- 플러그인들과의 호환에 어려움이 있다.